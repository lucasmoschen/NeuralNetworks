{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks \n",
    "\n",
    "Specialized for processing data with known grid-like topology. Examples include time-series and image data. It employs the **convoltution** operator. \n",
    "\n",
    "Some important characteristcs are: sparse interactions, parameter sharinng and equivariant representations. \n",
    "\n",
    "## The Convolution Operator \n",
    "\n",
    "It's an operationg on two funtions:\n",
    "$$\n",
    "s(t) = (x * w)(t) := \\int x(a)w(t-a)da\n",
    "$$\n",
    "We call $x$ input, $w$ as the kernel and the output as feature map. \n",
    "\n",
    "Em machine learning, $x$ Ã© usualmente um tensor. For example, if we are dealing with a two-dimensional imagem $I$ as input, \n",
    "$$\n",
    "S(i,j) = (I*K)(i,j) = \\sum_m\\sum_n I(m,n)K(i-m,j-n)\n",
    "$$\n",
    "Many neural networks implement a related function called cross-correlation, that flips the kernel.\n",
    "\n",
    "When we are handling with a discrete convolution, we can see it as a matrix product. \n",
    "\n",
    "- Sparse interactions: The kernel is usually very smaller than the input, but can detect meaninful features and, with that in mind, decreases the nessecity of memory for the parameters. \n",
    "\n",
    "- Parameter sharing: Uses the same parameter for more than one funciton in a model. So, instead of learning local parameters, we learn only one set. It doesn't reduce the computation, but it does reduce the storage requirement. \n",
    "\n",
    "- Equivariance to translation: if the input changes, the output changes in the same way. That is, $f(x)$ is equivariant to a function $g$ if $f(g(x)) = g(f(x))$. In this case, if $g$ is a translation, the convolution will be equivariant to it. \n",
    "\n",
    "## Pooling \n",
    "\n",
    "A typical layer of a convolutional network consists on three stages. In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations. In the second state, each linear activations passes through a nonlinear activation function (detector stage) and in the third stage we use a pooling function. The latter replaces the output of the net at a certain location with a summary statistic of the nearby outputs. FOr example *max pooling* reports the maximum output within a rectangular neighborhood. It helps the representation to be invariant to small translations of the input. \n",
    "\n",
    "> Padding: used to keep input and output as the same size, because, when we apply the kernel, it reduces the size. \n",
    "\n",
    "> Striding: controls how much the filters move \n",
    "\n",
    "\n",
    "## Major Architectures\n",
    "\n",
    "1. LeNet-5: simplest architectures. 2 convolutional and 3 fully-connected. Average-Pooling Layer. About 60 thousand parameters. \n",
    "\n",
    "2. AlexNet: 5 convolutional and 3 fully connected. Implemented ReLus as activate functions. About 60 M parameters.\n",
    "\n",
    "3. VGG-16: 13 convolutional and 3 fully connected layers. It uses smaller size filters. It' about 148 M parameters. Its contribution is the designing of deeper networks. \n",
    "\n",
    "4. Inception-v1: Network in network approach (instead of representing a pixel as a linear combination of weights and current sliding window, there is a mini neural network with 1 hidden layer) is used. It has parallel towers of convolutional filters followed by concatenation. About 5 M parameters. \n",
    "\n",
    "5. Inception-v3: Among the first designers to use batch normalization and some improvements over the previous one. As to represent big size filters as a series of small ones. Over 24 M parameters \n",
    "\n",
    "6. ResNet-50: The basic building block for ResNets are the conv and identity blocks. Because they look alike, you might simplify ResNet-50 like this \n",
    "\n",
    "And other!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neural Network)",
   "language": "python",
   "name": "neunet_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
