{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Networks \n",
    "\n",
    "Also called **multilayer perceptron** (MLPs). The goal is to approximate some function $f^*$. In order to do that, it defines a mapping $y = f(x;\\theta)$ and learns the parameters $\\theta$ that result in the best function approximation. It's called **feedforward** because information flows through the function from $x$ to the output $y$.   \n",
    "\n",
    "The model is associated with directed acyclic graph and describes how the functions are composed together. We can form, for example, $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. In this case, $f^{(i)}$ is called $i^{th}$ layer. The overall length of the chain gives the **depth** of the model. In the training, we drive $f(x)$ to match $f^{*}(x)$. Because the training dara does not show the desired outut for the internal layers, they are called **hidden**. \n",
    "\n",
    "The dimensionality of these hidden layers determines the **width** of the model. Each element of the vector in the layer \"plays\" a role analogous to a neuron. \n",
    "\n",
    "Starting from the linear models (limited to linear functions), we think in extend them to represent nonlinear functions of $x$. We can apply the linear model not to $x$, but to a transformed input $\\phi(x)$, where $\\phi$ is nonlinear. The strategy of deep learninng is to learn $\\phi$. In this approach, we have a model\n",
    "\n",
    "$$\n",
    "y = f(x;\\theta, w) = \\phi(x;\\theta)^Tw\n",
    "$$\n",
    "\n",
    "We parametrize the $\\phi$ representation and find $\\theta$ that corresponds to a good representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "\n",
    "![model1](images/model-xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hidden layer $h$, we must use a nonlinear function. Most neural networks do so using an affinne transformation controlled by learned parameters followed by a fixed nonlinear function, called activation function, that is \n",
    "\n",
    "$$\n",
    "h = g(W^Tx + c)\n",
    "$$\n",
    "\n",
    "In modern neural networks the default recommendation is to use **rectified linear unit** or ReLU, defined by $g(z) = max\\{0,z\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Based Learning\n",
    "\n",
    "The difference from linear models is that the loss functions become nonconvex in neural networks. Convex optimization converges starting from any initial parameters. However, stochastic gradient descent applied to nonconvex loss functions has no such convergence guarantee and is sensitive to the values of the initial parameters. For feedforward neural networks it's important to initialize all weights to small random values. The biases may be initialized to zero or small. \n",
    "\n",
    "### Cost Functions \n",
    "\n",
    "In most cases, our parametric model defines a distribution $p(y|x;\\theta)$, and for that we user the principle of maximum likelihood, through th cross entropy. \n",
    "\n",
    "$$\n",
    "H(p,q) = -E_p[\\log q]\n",
    "$$\n",
    "\n",
    "Sometimes we predict some statistic of $y$ conditioned on $x$. \n",
    "\n",
    "We can add a regularization term also. \n",
    "\n",
    "#### Learning Conditional Distributions with Maximum Likelihood\n",
    "\n",
    "Most modern neural networks uses maximization of the likelihood. \n",
    "\n",
    "$$\n",
    "J(\\theta) = -E_{x,y\\sim\\hat{p}_{data}}\\log p_{model}(y|x)\n",
    "$$\n",
    "\n",
    "An advantage of this approach of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model. The gradient of the cost function must be large and predictable enought to serve as a good guide for the learning algorithm. \n",
    "\n",
    "#### Learning Conditional Statistics \n",
    "\n",
    "We can see the cost function as a functional rather than just a function. In this case we choose a function rather than a set of parameters. It requires **calculus of variations**. \n",
    "\n",
    "## Output Units\n",
    "\n",
    "We suppose that the feedforward network provides a set of hidden features defined by $h = f(x;\\theta)$. The role of the output layer is then to provide some addicional transformation from the features to complete the task that the network must perform. \n",
    "\n",
    "### Linear Units for Gaussian Output Distributions\n",
    "\n",
    "Affine transformation. Given features $h$, a layer of linear output produces $\\hat{y} = W^Th + b$. Often used to produce the mean of a conditional Gaussian distribution. \n",
    "\n",
    "### Sigmoid Units for Bernoulli Output Distributions\n",
    "\n",
    "It's defined as: \n",
    "\n",
    "$$\\hat{y} = \\sigma(w^Th + b),$$\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "### Softmax Units for Multinoulli OUtput Distributions \n",
    "\n",
    "Now we need to produce a vector $\\hat{\\vec{y}}$ with $\\hat{y}_i = P(y = i|x)$. \n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "We wish to maximize $\\log P(y = i; z)$, so:\n",
    "\n",
    "$$\n",
    "\\log \\text{softmax}(z)_i = z_i - \\log\\sum_j \\exp(z_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Units\n",
    "\n",
    "### ReLU and Their Generalizations \n",
    "\n",
    "It uses the activation function $g(z) = z^+$. Whenever the unit is active, the derivatives remain large. They are typicalli used on yop of affine transformation. We can generalize with $g(\\,\\alpha)_i = z_i^+ + \\alpha_i z_i^-$. **Absolute value rectification** puts it $\\alpha_i = -1$, while **leaky ReLU** a small value. **Parametric ReLU** treats it as a learnable paramenter. **Maxout units** generalize further with $g(z)_i = \\max_{j \\in G^{(i)}} z_j$, where $G^{(i)}$ is the set of indices $\\{(i-1)k+1,...,ik\\}$. The latter can learn the activation function (as long as it's convex). \n",
    "\n",
    "### Logistic Sigmoid and Hyperbolic Tangent\n",
    "\n",
    "These activation functions are closely related because $\\tanh(z) = 2\\sigma(2z) - 1$. It's discouraged to use them in hidden units, because of the saturation. \n",
    "\n",
    "### Other Hidden Units\n",
    "\n",
    "- Radial Basis Function: $h_i = \\exp\\left(-\\frac{1}{\\sigma_i^2}||W_{:,i} - x||^2\\right)$. It may be difficult to optimize. \n",
    "\n",
    "- Softplus: $g(a) = \\zeta(a) = \\log(1 + e^a)$. A smooth version of rectifier. It's use is discouraged. It doesn't improve the results. \n",
    "\n",
    "- Hard tanh: $g(a) = \\max(-1,\\min(1,a)) = a\\mathbb{1}(|a|\\leq 1) + \\text{sign(a)}\\mathbb{1}(|a| > 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Design \n",
    "\n",
    "It referes to the overall structure of the network: how many units, hoot to connect the units to each other. In the common structure, with have:\n",
    "\n",
    "$$\n",
    "h^{(i+i)} = g^{(i+1)}(W^{(i+1)T} h^{(i)} + b^{(i+1)}), h^{(0)} = x\n",
    "$$\n",
    "\n",
    "### Universal Approximation Properties and Depth \n",
    "\n",
    "It uses the **universal approximation theorem** that states the feedforward network with a linear output layer and at least one hidden layer with any squashing activation function can approximate any Borel menasuable function from one finite dimensional space with any desired nonzero error, provided enoughh hidden units. We are not guaranteed that the trainging algorithm will be able to learn that function. In many circunstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error. \n",
    "\n",
    "We may choose a deep model for statistical reasons. Any time we choose a specific machine learning algorithmm, we are implicitly stating some set of prior beliefs. A deep model encodes a very general belief. \n",
    "\n",
    "### Other Considerations \n",
    "\n",
    "How to connect a pair of layers to each other. The default is described by a linear transformation $W$ and every input unit is connected to every output unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation \n",
    "\n",
    "The **back propagation** algorithm allows the information from the cost to the flow backward through the etwork in order to compute the gradient. It refers to the method for computing the gradient, and another algorithm is used to perform learning using this gradient. In learning algorithms we are interested in $\\nabla_{\\theta} J(\\theta)$. \n",
    "\n",
    "### Recursively Applying the Chain Rule \n",
    "\n",
    "We have a choice to make: recalculate some times or store these subexpressions. First consider a computational graph describing how to compute a single scalar $u^{(n)}$ (quantity gradient we want to obtain). We assume the nodes of the graph have been ordered. Each node $u^{(i)}$ is associated with an operation $f^{(i)}$ and is computed evaluating \n",
    "\n",
    "$$\n",
    "u^{(i)} = f(A^{(i)}),\n",
    "$$\n",
    "\n",
    "where $A^{(i)}$ is the set of all nodes parents of $u^{(i)}$. To perform back propagation we construct a computational graph with some extra nodes. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u^{(n)}}{\\partial u^{(j)}} = \\sum_{i:j\\in Pa(u^{(i)})} \\frac{\\partial u^{(n)}}{\\partial u^{(i)}}\\frac{\\partial u^{(i)}}{\\partial u^{(j)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
