{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Deep Learning \n",
    "\n",
    "## Parameter Norm Penalties \n",
    "\n",
    "We typically choose to use a parameter norm penalty $\\Omega$ that penalizes only the weights of the aﬃne transformation at each layer andleaves the biases unregularized. \n",
    "\n",
    "### $L^2$ Parameter Regularization\n",
    "\n",
    "This regularization strategy drives the weights closer to the origin by adding a regularization term $\\Omega(\\theta) = \\frac{1}{2}||w||^2$ to the objective function. In other academic communities, $L^2$ regularization is also known as ridge regression or Tikhonov regularization.\n",
    "\n",
    "$$\n",
    "\\tilde{J}(w; X, y) = \\frac{\\alpha}{2}w^Tw + J(w; X,y)\n",
    "$$\n",
    "\n",
    "The gradient update is, therefore, \n",
    "$$\n",
    "w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w; X,y))\n",
    "$$\n",
    "\n",
    "The effect can be seen in the eigenvalues of the Hessian Matrix. If some of them is very less than $\\alpha$, the regularization parameter, it will vanish this component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L^1$ Parameter Regularization\n",
    "\n",
    "In this case, the regularization is defined as $\\Omega(\\theta) = ||w||_1$. So:\n",
    "\n",
    "$$\n",
    "\\tilde{J}(w, X, y) = \\alpha||w||_1 + J(w, X, y)\n",
    "$$\n",
    "\n",
    "It turns the solution more sparse than the first regularization. It's used as feature selection, thus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Augmentation\n",
    "\n",
    "Create fake data based on the current dataset, in order to improve the results. In the classfication task, it's easier, because we can generate new pairs of data transforming the input. In the case of images, transformation as translation, rotating and scaling have proved to be quite effective. Neural networks prove not to be very robus to noise, however. One way to solve ir is apply random noise in the input. \n",
    "\n",
    "## Noise Robustness\n",
    "\n",
    "Noise applied to the hidden units is an important topic. The dropout algorithm is the main development of that approach. Noise can be added in the weights. It's a stochastic implementation of Bayesian inference over weights. Under some assumptions, apply noise to the weights is equivalent to move a regularization.\n",
    "\n",
    "## Early Stopping \n",
    "\n",
    "Every time the error on the validation setimproves, we store a copy of the model parameters. When the training algorithmterminates, we return these parameters, rather than the latest parameters.\n",
    "\n",
    "## Parameter Tying and Parameter Sharing \n",
    "\n",
    "The sharing is to force sets of parameters to be equal. It can reduce the memory footprint on the model. \n",
    "\n",
    "## Bagging and Ensemble Methods\n",
    "\n",
    "Bagging is a techinique for reducing generalization error by combining several models. Training several models and after wach one will vote in classification tasks. The reasin that it works is that different models will usually not make the same errors on the test set. Bagging consists in changing the dataset through the bootstrap approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "It trains the ensemble consisting of all subnetworks that can be formed by removing nonoutput units Effectively, multiplying its output value by zero. Each time we load an example into a minibatch, werandomly sample a diﬀerent binary mask to apply to all the input and hiddenunits in the network. The mask for each unit is sampled independently from allthe others. A tiny fraction of the possible subnetworks are each trained for a single step, and the parameter sharing causes the remaining subnetworks to arrive at good settings of the parameters.\n",
    "\n",
    "The prediction is given by\n",
    "\n",
    "$$\n",
    "\\sum_{\\mu} p(\\mu)p(y|x,\\mu)\n",
    "$$\n",
    "\n",
    "We can approximate the inferece with sampling, by averaging from many masks. \n",
    "\n",
    "We can improve the results using the geometric mean. \n",
    "\n",
    "$$\n",
    "p_{emsemble}(y|x) \\propto \\left(\\prod_{\\mu} p(y|x,\\mu)\\right)^{2^{-d}}\n",
    "$$\n",
    "\n",
    "where $d$ is the number of units that may be dropped. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
